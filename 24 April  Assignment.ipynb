{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597d21a1-dc2c-4a7c-84c7-adb5fc4dd841",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is a projection and how is it used in PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1313427-045d-47dc-a268-07f5a01049d7",
   "metadata": {},
   "source": [
    "ANS -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1a1d62-e8ab-4510-be51-27cac8519b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "A projection is a transformation of data from one space to another. In PCA, the projection is used to transform the data into a \n",
    "lower-dimensional space called principal components (PCs).\n",
    "The goal of PCA is to find the best summary of the data using a limited number of PCs. \n",
    "The PCs are chosen in such a way that they capture as much variation in the data as possible. \n",
    "The first PC captures the most variation, followed by the second PC, and so on. \n",
    "The projection is done by finding the eigenvectors of the covariance matrix of the data and projecting the data onto these eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1a8ed7-83c0-4fd3-99d3-a158fe46f4da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d60c59-3e65-48ab-b233-64eb5a172bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. How does the optimization problem in PCA work, and what is it trying to achieve?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0242d2-b741-402d-b1df-a4a9290a1fc3",
   "metadata": {},
   "source": [
    "ANS -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd633ac9-bf98-4371-a268-58d6fbb0754c",
   "metadata": {},
   "outputs": [],
   "source": [
    "The optimization problem in PCA is to find the principal components that capture the most variation in the data.\n",
    "The first principal component captures the most variation, followed by the second principal component, and so on. \n",
    "The optimization problem is solved by finding the eigenvectors of the covariance matrix of the data. \n",
    "The eigenvectors are then used to project the data onto a lower-dimensional space. \n",
    "The goal of PCA is to reduce the dimensionality of the data while preserving as much of the variation in the data as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618c31cf-5b12-4b3b-ba5e-c9aac5ea34d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8b0e6b-8cac-463c-939d-d972166a3b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. What is the relationship between covariance matrices and PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94996afa-2958-48a0-952a-2e0028a89589",
   "metadata": {},
   "source": [
    "ANS -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb02cca7-b0fe-43e9-966f-85d5f0ae52fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "The covariance matrix is used in PCA to find the principal components 12. The covariance matrix is a p × p symmetric matrix\n",
    "(where p is the number of dimensions) that has as entries the covariances associated with all possible pairs of the initial variables.\n",
    "The correlation matrix is also used in PCA when variables are on different scales. Using the correlation matrix is equivalent to\n",
    "standardizing each of the variables (to mean 0 and standard deviation 1).In general, PCA with and without standardizing will give \n",
    "different results ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff115a8-57f7-4b5a-b98b-cc9aef0d282d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d791da2-d2d8-41f7-ae50-1456f0a8feb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. How does the choice of number of principal components impact the performance of PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5e7718-6946-46ce-8e88-1195ac4c9c35",
   "metadata": {},
   "source": [
    "ANS -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89126d0-4a2e-45ef-a96c-1b2a4bbedb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "The optimal number of principal components in PCA is reached when the cumulative variance stops growing fast. \n",
    "The minimum number of principal components required to preserve 95% of the data’s variance can be computed using a specific command. \n",
    "Another technique to decide on the number of components to retain in the analysis is the permutation test, which compares the explained\n",
    "variances in the original PCA with the explained variances in the PCAs rerunning on the dataset after several data permutations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a768b467-9f43-4890-80dd-a27966ab192e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcba922c-97a3-41ea-9087-71a531bd7175",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. How can PCA be used in feature selection, and what are the benefits of using it for this purpose?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be8e114-bee0-4ec3-aedf-0ef3bfe609f2",
   "metadata": {},
   "source": [
    "ANS -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e354ed0-1693-4a48-93ac-2b62398cc16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA can be used in feature selection by determining the most crucial features or components and reducing the dimensionality of the data, \n",
    "which is one of its primary benefits. PCA can also be used to derive new features or elements from the original data that might be more\n",
    "insightful or understandable than the original features.\n",
    "\n",
    "PCA offers a number of benefits such as reduction of noise in the data, feature selection (to a certain extent), and the ability to produce\n",
    "independent, uncorrelated features of the data. By reducing the dimensionality of the data, PCA enables us to better generalize machine \n",
    "learning models. This helps us deal with the “curse of dimensionality”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7b40d4-4797-47d0-8047-5c14ecbcd11f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df472e4a-6265-4b9d-97ee-b8c1cb97fdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. What are some common applications of PCA in data science and machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575c9d53-8f10-4116-94dd-518d8512b454",
   "metadata": {},
   "source": [
    "ANS -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d087491-fae2-4742-abb5-bdded436dcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA (Principal Component Analysis) is a commonly used unsupervised machine learning algorithm that has a variety of applications.\n",
    "Some of the applications of PCA in machine learning include:\n",
    "\n",
    "1.Exploratory data analysis\n",
    "2.Dimensionality reduction\n",
    "3.Information compression\n",
    "4.Data de-noising\n",
    "5.And plenty more.\n",
    "\n",
    "PCA is predominantly used as a dimensionality reduction technique in domains like facial recognition, computer vision and image compression.\n",
    "It is also used for finding patterns in data of high dimension in the field of finance, data mining, bioinformatics, psychology, etc.\n",
    "\n",
    "If you want to learn more about PCA and its applications in machine learning, you can check out this blog post which provides a detailed \n",
    "guide on PCA for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908490b6-b657-461c-a1c4-8892f54b44af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dacaa0-f1b4-4846-9ba2-262f2c36a9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7.What is the relationship between spread and variance in PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bccad78-a801-4617-88d3-59b2558299fe",
   "metadata": {},
   "source": [
    "ANS -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b5e8e2-1c7f-4b0a-965b-c4285578260c",
   "metadata": {},
   "outputs": [],
   "source": [
    "In PCA, variance is the spread of the data. More the variance more information is contained. The direction represents across which principal\n",
    "axes the data is mostly spread out or has most variance. The magnitude signifies the amount of variance that Principal Component captures \n",
    "of the data when projected onto that axis. The principal components are a straight line, and the first principal component holds the most\n",
    "variance in the data. The variance explained can be understood as the ratio of the vertical spread of the regression line to the vertical\n",
    "spread of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a654ef3-01f4-4429-a508-091a4741d204",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5677a6-5822-43c7-8773-14e8dcecedb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. How does PCA use the spread and variance of the data to identify principal components?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f755feb1-2f6f-4c85-97d5-fdd906ab0474",
   "metadata": {},
   "source": [
    "ANS -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92e6852-0005-44cb-bdc4-f96c9334aaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA is a powerful mathematical technique to reduce the complexity of data. It detects linear combinations of the input fields that can\n",
    "best capture the variance in the entire set of fields, where the components are orthogonal to and not correlated with each other. \n",
    "PCA works by identifying the directions that capture the most variation in the data and projecting the data onto those directions, \n",
    "which are called principal components. The eigenvectors and eigenvalues of the covariance matrix are computed to identify the principal\n",
    "components. A feature vector is created to decide which principal components to keep. The data is then recast along the principal\n",
    "components axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83a237e-055b-455e-953a-3bd11fc78311",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e644e947-c072-4b6e-962e-52fc1723fb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9. How does PCA handle data with high variance in some dimensions but low variance in others?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a456c081-4d55-422e-b5a4-3af68e209590",
   "metadata": {},
   "source": [
    "ANS -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589342fc-3777-4813-990f-4e837f724b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA gives more weight to variables that have higher variances than variables with low variances. \n",
    "Therefore, it is important to normalize the data on the same scale to get a reasonable covariance.\n",
    "PCA transforms your data into a number of dimensions whose data are completely different from the original ones.\n",
    "Principal component analysis (PCA) is a technique that transforms high-dimensional data into lower dimensions while retaining\n",
    "as much information as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e06ca4d-a54b-438b-bbd8-86e4f33e2438",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fae3727-4ca4-4f68-8f07-fb559a3aa75e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c828096-4694-4d82-855b-f2f604d3b9f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
